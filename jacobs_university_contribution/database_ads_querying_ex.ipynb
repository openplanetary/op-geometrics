{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess as sub\n",
    "import sys\n",
    "import ads\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import ogr\n",
    "from matplotlib import pylab\n",
    "from paper_class import Paper\n",
    "from feature_class import Feature\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# routine to read information from json file\n",
    "# creates a Feature instance with extracted data\n",
    "def feature_extract(obj):\n",
    "    if '__type__' in obj and obj['__type__'] == 'Feature':\n",
    "        return Feature(obj['name'], obj['id'], obj['polygon_coordinates'], obj['publications'])\n",
    "    return Feature(\"None\", 0, \"None\", \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# routine to parse json file and fetch definition of each feature\n",
    "def parse_json_stream(text):\n",
    "    list_idx = [m.start() for m in re.finditer('}\\n', text)]\n",
    "    list_objs = []\n",
    "    last_index = 0\n",
    "    for index in list_idx:\n",
    "        new_feature = json.loads(text[last_index:index+2], object_hook=feature_extract)\n",
    "        if (new_feature.name == \"None\"):\n",
    "            break\n",
    "        list_objs += [new_feature]\n",
    "        last_index = index+2\n",
    "    return list_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code is taken from jupyter notebook \n",
    "# https://github.com/epn-vespa/vespamap17-hackathon/blob/master/vespa-mapping-jupyter-samp/SendName_to_IAUnomenclature_findPolygon_hacky.ipynb\n",
    "def draw_feature_contour(feature):\n",
    "    geomOGR = ogr.CreateGeometryFromWkt(feature.pcoord)\n",
    "    \n",
    "    figsize(6,6)\n",
    "    \n",
    "    coords = json.loads(geomOGR.ExportToJson())['coordinates'][0]\n",
    "    x = [i for i,j in coords[0]]\n",
    "    y = [j for i,j in coords[0]]\n",
    "\n",
    "    plot(x, y, 'b')\n",
    "    pylab.xlabel('Longitude')\n",
    "    pylab.ylabel('Latitude')\n",
    "    pylab.title('Feature : ' + feature.name)\n",
    "    grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To create a database with features and their topological structure\n",
    "# One needs to create a csv file with \n",
    "# https://planetarynames.wr.usgs.gov/nomenclature/AdvancedSearch\n",
    "# and pass it to the database_creation script\n",
    "# IMPORTANT: feature ID and feature name should be present in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "script_name = \"database_merged.py\"\n",
    "csv_file_name = \"Mars_short.csv\"\n",
    "output_file_name = \"features.json\"\n",
    "try:\n",
    "    ret = sub.call(' '.join([\"python\", script_name, csv_file_name, output_file_name]), shell = True)\n",
    "    if (ret != 0):\n",
    "        print(\"Child returned\", ret, file=sys.stderr)\n",
    "except OSError as e:\n",
    "    print(\"Execution failed: \", e, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now database is built and we want to provide speific queries to ADS\n",
    "authors = [\"Tanaka, K. L.\", \"Kolb, E. J.\", \"Fortezzo, C.\"]\n",
    "toponym = \"Abalos Colles\"\n",
    "years = range(2003, 2008)\n",
    "query_list = [\"\", \"\", \"\"]\n",
    "\n",
    "if (toponym):\n",
    "    query_list[0] = toponym\n",
    "\n",
    "if (authors):\n",
    "    for idx in range(len(authors) - 1):\n",
    "        query_list[1] += \"author:\" + authors[idx] + \" OR \"\n",
    "    query_list[1] += \"author:\" + authors[-1]\n",
    "        \n",
    "if (years):\n",
    "    query_list[2] = \"year:[\" + str(years.start) + \" TO \" + str(years.stop) + \"]\" \n",
    "\n",
    "query = ' AND '.join(filter(None, query_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not query):\n",
    "    raise RuntimeError(\"Empty query is not allowed!\")\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#performing a query to ADS API\n",
    "example_results = []\n",
    "try:\n",
    "    example_results = list(ads.SearchQuery(q=query, fl=['title', 'author', 'year', 'pub', 'bibcode']))\n",
    "except (ads.exceptions.APIResponseError, ads.exceptions.SolrResponseParseError) as e:\n",
    "    \"Error: {}\".format(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# representing results with custom Paper class\n",
    "results_new = []\n",
    "for paper in example_results:\n",
    "    results_new.append(Paper(paper.title[0], paper.author, paper.year, paper.pub, paper.bibcode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dig the database for the requested toponym\n",
    "with open(output_file_name) as fin:\n",
    "    features = parse_json_stream(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the entry exists - draw the toponym as a polygon\n",
    "for feature in features:\n",
    "    if (toponym == feature.name):\n",
    "        print(toponym + \" is found in the database!\")\n",
    "        draw_feature_contour(feature)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create pandas table\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "df = pd.DataFrame([paper.to_dict() for paper in results_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
